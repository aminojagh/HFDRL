{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminojagh/HFDRL/blob/main/notebooks/unit3/unit3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 3: Deep Q-Learning with Atari Games üëæ using RL Baselines3 Zoo"
      ],
      "metadata": {
        "id": "CAWQLzuypjbs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7xBVPzoXxOg"
      },
      "source": [
        "In this notebook, **you'll train a Deep Q-Learning agent** playing Space Invaders using [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo), a training framework based on [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "We're using the [RL-Baselines-3 Zoo integration, a vanilla version of Deep Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) with no extensions such as Double-DQN, Dueling-DQN, and Prioritized Experience Replay.\n",
        "\n",
        "‚¨áÔ∏è Here is an example of what **you will achieve** ‚¨áÔ∏è\n",
        "\n",
        "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n",
        "\n",
        "To validate this hands-on for the certification process, you need to push your trained model to the Hub and **get a result of >= 200**.\n",
        "\n",
        "To find your result, go to the leaderboard and find your model, **the result = mean_reward - std of reward**\n",
        "\n",
        "For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéÆ Environments:\n",
        "\n",
        "- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n",
        "\n",
        "You can see the difference between Space Invaders versions here üëâ https://gymnasium.farama.org/environments/atari/space_invaders/#variants\n",
        "\n",
        "### üìö RL-Library:\n",
        "\n",
        "- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"
      ],
      "metadata": {
        "id": "ykJiGevCMVc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An advice üí°\n",
        "It's better to run this colab in a copy on your Google Drive, so that **if it timeouts** you still have the saved notebook on your Google Drive and do not need to fill everything from scratch.\n",
        "\n",
        "To do that you can either do `Ctrl + S` or `File > Save a copy in Google Drive.`\n",
        "\n",
        "Also, we're going to **train it for 90 minutes with 1M timesteps**. By typing `!nvidia-smi` will tell you what GPU you're using.\n",
        "\n",
        "And if you want to train more such 10 million steps, this will take about 9 hours, potentially resulting in Colab timing out. In that case, I recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."
      ],
      "metadata": {
        "id": "Nc8BnyVEc3Ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install RL-Baselines3 Zoo and its dependencies üìö\n",
        "\n",
        "If you see `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.` **this is normal and it's not a critical error** there's a conflict of version. But the packages we need are installed."
      ],
      "metadata": {
        "id": "wS_cVefO-aYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "!pip install rl_zoo3"
      ],
      "metadata": {
        "id": "S1A_E4z3awa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig cmake ffmpeg"
      ],
      "metadata": {
        "id": "8_MllY6Om1eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S9mJiKg6SqC"
      },
      "source": [
        "To be able to use Atari games in Gymnasium we need to install atari package. And accept-rom-license to download the rom files (games files)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]\n",
        "# !pip install gymnasium[accept-rom-license]\n",
        "!pip install AutoROM.accept-rom-license"
      ],
      "metadata": {
        "id": "NsRP-lX1_2fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a virtual display üîΩ\n",
        "\n",
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install the librairies and create and run a virtual screen üñ•"
      ],
      "metadata": {
        "id": "bTpYcVZVMzUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "BE5JWP5rQIKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train our Deep Q-Learning Agent to Play Space Invaders üëæ"
      ],
      "metadata": {
        "id": "2FRwp_WHvT1Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPgzluo9z-u"
      },
      "source": [
        "To train an agent with RL-Baselines3-Zoo, we just need to do two things:\n",
        "\n",
        "1. Create a hyperparameter config file that will contain our training hyperparameters called `dqn.yml`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat << 'EOF' > dqn.yml\n",
        "SpaceInvadersNoFrameskip-v4:\n",
        "  env_wrapper:\n",
        "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
        "  frame_stack: 4\n",
        "  policy: 'CnnPolicy'\n",
        "  n_timesteps: !!float 1e6\n",
        "  buffer_size: 100000\n",
        "  learning_rate: !!float 1e-4\n",
        "  batch_size: 32\n",
        "  learning_starts: 100000\n",
        "  target_update_interval: 1000\n",
        "  train_freq: 4\n",
        "  gradient_steps: 1\n",
        "  exploration_fraction: 0.1\n",
        "  exploration_final_eps: 0.01\n",
        "  # If True, you need to deactivate handle_timeout_termination\n",
        "  # in the replay_buffer_kwargs\n",
        "  optimize_memory_usage: False\n",
        "EOF"
      ],
      "metadata": {
        "id": "xJtIsmOLvIkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VjblFSVDQOj"
      },
      "source": [
        "Here we see that:\n",
        "- We use the `Atari Wrapper` that preprocess the input (Frame reduction ,grayscale, stack 4 frames)\n",
        "- We use `CnnPolicy`, since we use Convolutional layers to process the frames\n",
        "- We train it for 10 million `n_timesteps`\n",
        "- Memory (Experience Replay) size is 100000, aka the amount of experience steps you saved to train again your agent with.\n",
        "\n",
        "üí° My advice is to **reduce the training timesteps to 1M,** which will take about 90 minutes on a P100. `!nvidia-smi` will tell you what GPU you're using. At 10 million steps, this will take about 9 hours, which could likely result in Colab timing out. I recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qTkbWrkECOJ"
      },
      "source": [
        "In terms of hyperparameters optimization, my advice is to focus on these 3 hyperparameters:\n",
        "- `learning_rate`\n",
        "- `buffer_size (Experience Memory size)`\n",
        "- `batch_size`\n",
        "\n",
        "As a good practice, you need to **check the documentation to understand what each hyperparameters does**: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn8bRTHvERRL"
      },
      "source": [
        "2. We start the training and save the models on `logs` folder üìÅ\n",
        "\n",
        "- Define the algorithm after `--algo`, where we save the model after `-f` and where the hyperparameter config is after `-c`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuocgdokSab9"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.train --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLomIiMKQaf"
      },
      "source": [
        "## Let's evaluate our agent üëÄ\n",
        "- RL-Baselines3-Zoo provides `enjoy.py`, a python script to evaluate our agent. In most RL libraries, we call the evaluation script `enjoy.py`.\n",
        "- Let's evaluate it for 5000 timesteps üî•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_uSmwGRSk0z"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liBeTltiHJtr"
      },
      "source": [
        "## Publish our trained model on the Hub üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppu9yePwHrZX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW436XnhHw1H"
      },
      "source": [
        "Let's run push_to_hub.py file to upload our trained agent to the Hub.\n",
        "\n",
        "`--repo-name `: The name of the repo\n",
        "\n",
        "`-orga`: Your Hugging Face username\n",
        "\n",
        "`-f`: Where the trained model folder is (in our case `logs`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HQNlAXuEhci"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga amin-oj  -f logs/\n",
        "# TODO: fails to upload the video replay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyRKcCYY-dIo"
      },
      "source": [
        "## Load a powerful trained model üî•\n",
        "- The Stable-Baselines3 team uploaded **more than 150 trained Deep Reinforcement Learning agents on the Hub**.\n",
        "\n",
        "You can find them here: üëâ https://huggingface.co/sb3\n",
        "\n",
        "Some examples:\n",
        "- Asteroids: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n",
        "- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n",
        "- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n",
        "- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n",
        "\n",
        "Let's load an agent playing Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-9QVFIROI5Y"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZQNY_r6NJtC"
      },
      "source": [
        "1. We download the model using `rl_zoo3.load_from_hub`, and place it in a new folder that we can call `rl_trained`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdBNZHy0NGTR"
      },
      "outputs": [],
      "source": [
        "# Download model and save it into the logs/ folder\n",
        "!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFt6hmWsNdBo"
      },
      "source": [
        "2. Let's evaluate if for 5000 timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOxs0rNuN0uS"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqaco8W-huW"
      },
      "source": [
        "## Some additional challenges üèÜ\n",
        "Also, **if you want to learn to implement Deep Q-Learning by yourself**, you definitely should look at CleanRL implementation: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}